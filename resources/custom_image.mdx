---
title: 'Deploying Custom Models'
description: 'How to build your own containerized inference engines and deploy'
icon: 'screwdriver-wrench'
---

The CentML Platform simplifies deploying your custom models and AI applications. Package your model or application as a containerized HTTP server, then deploy it on CentML using the General Inference deployment option.

Here’s an example of how to build a stable diffusion model served by a FastAPI-based inference server: https://github.com/CentML/codex/tree/main/general-apps/general-inference/stable-diffusion/centml-endpoint

<Note>
By default, Docker containers run as the root user. However, the CentML Platform restricts running containers as root for security reasons. To address this, ensure that you create the container image using a non-root numeric user.

Here’s an example of how you can configure this in a Dockerfile,
```docker
# Create a new user to run the application required for centml endpoints
ARG USERNAME="centml"
RUN useradd -u 1024 -m -d /home/${USERNAME} -s /bin/bash ${USERNAME} && chown -R ${USERNAME}:${USERNAME} /home/${USERNAME}
USER 1024
```
</Note>

<Note>
If you are building the container on MacOS, make sure to set the image platform as `linux/amd64`.
```bash
docker build --platform linux/amd64 -t stable-diffusion:latest .
```
</Note>

After building the container, push it to a public container registry, such as [Docker Hub](https://hub.docker.com/). Then, navigate to the General Inference option in the CentML Platform and enter the image name, tag, and container port to deploy your application.


## What's Next

<CardGroup cols={2}>
  <Card
    title="Agents on CentML"
    icon="user-secret"
    href="/resources/json_and_tool"
  >
    Learn how agents can interact with CentML services.
  </Card>
  <Card
    title="Clients"
    icon="terminal"
    href="/clients/setup"
  >
    Learn how to interact with platform programmatically
  </Card>
  <Card
    title="Resources and Pricing"
    icon="coins"
    href="/resources/pricing"
  >
    Learn more about CentML Pricing
  </Card>
    <Card
    title="Get Support"
    icon="headset"
    href="/resources/requesting_support"
  >
    Submit a Support Request
 </Card>
  <Card
    title="CentML Serverless Endpoints"
    icon="cloud-binary"
    href="/apps/serverless"
  >
    Dive deeper into advanced Serverless configurations and patterns.
 </Card>
  <Card
    title="LLM Serving"
    icon="messages"
    href="/apps/llm"
  >
    Explore dedicated public and private endpoints for production model and model infrastructure management.
 </Card>
</CardGroup>
