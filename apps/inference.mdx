---
title: 'General Inference'
description: 'Deploy and scale any model'
icon: 'circle-nodes'
---

We make it easy for you to containerize and deploy any custom models on our platform. In addition, we also provide popular inference engines like vLLM and Ollama.

## 1. Configure your inference deployment

Select or enter a container inference engine image, tag and container port.

<Frame>
    <img src="/images/inf_1.png" style={{ borderRadius: '0.5rem' }} />
</Frame>

<Tip>
To make the endpoint private, select the "Make it a private endpoint?" option. This will automatically generate a TLS certificate, which you can download as a .pem file after the deployment is complete. Only those with this TLS certificate will have access to the endpoint. For more details, please refer to this [resource](/resources/private).
</Tip>

For further customization, you can configure the optional fields as well.

- **backend protocol:** Select the protocol for your deployment - `HTTP` (default) or `GRPC`. Choose gRPC if your inference engine exposes a gRPC service.
- **healthcheck:** Health check endpoint to verify the inference engine is ready. For HTTP deployments, this is an HTTP endpoint (default is `/`). For gRPC deployments, the platform uses TCP socket checks automatically, so this field is ignored. Native gRPC health checks are planned for future releases.
- **command and arguments:** Entrypoint command and arguments to run when the container starts. Default is entrypoints specified in the container image.
- **autoscaling:** Set the min and max scale for your deployment. We scale up and down your deployment to match the traffic based on the max concurrency set. Max concurrency refers to the maximum number of in-flight requests per replica. Default is infinity.
- **environment variables:** Pass in any additional environment variables to the container. e.g, HF_TOKEN

<Tip>
For building your own custom container image and deploying on CentML Platform, please refer to this [resource](/resources/custom_image).
</Tip>

## 2. Select the cluster and hardware to deploy
By default, CentML Platform provides several managed clusters and GPU instances for you to deploy your inference containers. 

<Frame>
    <img src="/images/inf_2.png" style={{ borderRadius: '0.5rem' }} />
</Frame>

Select the regional cluster and hardware instance that best fits your need and click Deploy.

<Tip>
You can integrate your own private cluster into the Platform through our bring-your-own-infrastructure support. To get started, please get in touch with us at support@centml.ai.
</Tip>


## 3. Monitor your deployment
Once deployed, you can see all your deployments under the listing view along with their current status.

<Frame>
    <img src="/images/inf_3.png" style={{ borderRadius: '0.5rem' }} />
</Frame>

Click on the deployment to view the details page, logs and monitoring information.

<Frame>
    <img src="/images/inf_4.png" style={{ borderRadius: '0.5rem' }} />
</Frame>

Once the deployment status is ready, the container port is going to be exposed under the endpoint url shown in the details page.

### Accessing your endpoint

All deployments are exposed externally on **port 443** with **TLS enforced**. The platform automatically provisions TLS certificates for your endpoint - no additional configuration is required.

| Protocol | Access URL | Description |
|----------|-----------|-------------|
| HTTP | `https://<endpoint_url>` | Standard HTTPS access (port 443 is implicit) |
| gRPC | `<endpoint_url>:443` | gRPC with TLS enabled |

<Note>
The **container port** you configure is used internally within the cluster. The platform's ingress layer handles TLS termination on port 443 and forwards requests to your container's internal port.
</Note>

**Example usage:**

```bash
# HTTP deployment
curl https://my-deployment.some-hash.cluster-alias.centml.com/v1/models

# gRPC deployment (using grpcurl)
grpcurl -d '{"prompt": "Hello"}' my-deployment.some-hash.cluster-alias.centml.com:443 inference.Service/Predict
```



# What's Next

<CardGroup cols={2}>
  <Card
    title="LLM Serving"
    icon="messages"
    href="/apps/llm"
  >
    Explore dedicated public and private endpoints for production model deployments.
 </Card>
  <Card
    title="Clients"
    icon="terminal"
    href="/clients/setup"
  >
    Learn how to interact with the CentML platform programmatically
  </Card>
  <Card
    title="Resources and Pricing"
    icon="coins"
    href="/resources/pricing"
  >
    Learn more about the CentML platform's pricing.
  </Card>
  <Card
    title="Private Inference Endpoints"
    icon="lock"
    href="/resources/private"
  >
Learn how to create private inference endpoints
 </Card>
  <Card
    title="Submit a Support Request"
    icon="headset"
    href="/resources/requesting_support"
  >
    Submit a Support Request.
 </Card>
  <Card
    title="Agents on CentML"
    icon="user-secret"
    href="/resources/json_and_tool"
  >
    Learn how agents can interact with CentML services.
  </Card>
</CardGroup>