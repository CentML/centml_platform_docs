---
title: Serverless Quickstart
description: Access CentML's Managed Models in Less Than a Minute. 
icon: rocket
---

CentML's serverless endpoints provide programmatic and browser-based access to popular models.

No need to worry about the underlying infrastructure, scaling deployments, or maintenance - just log in to start testing and evaluating models to determine how they fit your business needs.

When you're ready to move to production, you can deploy dedicated LLM endpoints that fit your performance requirements and budget, with CentML's [LLM Serving](https://docs.centml.ai/apps/llm).


## 1. Log into the CentML Platform
To access a CentML serverless endpoint, you need to log in to the CentML platform. To do so, navigate to https://app.centml.com in your browser and create a CentML user account or log in with your account's credentials if you already have one.
<Frame>
    <img src="/images/centml-signin.png" style={{ borderRadius: '0.5rem' }} />
</Frame>
### Automatic Sign-up and Account Creation

Sign in with Google or GitHub by clicking `Continue with Google` or `Continue with GitHub` and follow the standard OAuth and permissions flow.

### Manual Sign-up and Account Creation

To manually create an account, follow these steps:

    1. Click the `Sign up` link, and enter your first name, last name, and email address. Click `Continue`.
    <Frame>
    <img src="/images/enter-username.png" style={{ borderRadius: '0.5rem' }} />
</Frame>
    2. Create and enter a [secure password](https://1password.com/password-generator) into the text box and then click `Continue`.
    <Frame>
    <img src="/images/create-password.png" style={{ borderRadius: '0.5rem' }} />
</Frame>
    3. Verify your email address by entering the code sent to the email that you signed up with. The message should be from `no-reply@centml.ai` and titled `Please verify your email address`.
     Once you enter the code sent to your inbox into the CentML UI, you will be automatically signed in.
    <Frame>
    <img src="/images/validate-email.png" style={{ borderRadius: '0.5rem' }} />
</Frame>


Once logged in, you will see the CentML Platform console home page, as shown below.

<Frame>
    <img src="/images/homepage.png" style={{ borderRadius: '0.5rem' }} />
</Frame>


## 2. Create an API key
When accessing a CentML Serverless endpoint, you must authenticate using a Serverless API key **even if you are using the chat UI.** (see below)

<Frame>
    <img src="/images/chat-error.png" style={{ borderRadius: '0.5rem' }} />
</Frame>

To generate an API key, 

From the sidebar, go to the `Account` page and then select the `Vault` tab. 

From there you can generate an API key by clicking `Generate`. 

You can delete your API keys by clicking `Delete` next to the desired API key.

<Frame>
    <img src="/images/api_token.png" style={{ borderRadius: '0.5rem' }} />
</Frame>

## 3. Use CentML's Serverless Endpoints

CentML's Serverless endpoints can be accessed via the chat UI, as well as programmatically using Python, JavaScript, and cURL commands.

### Accessing the Chat UI

To access the chat UI, select the `Serverless` option from the sidebar menu.

### Configuring the Endpoint

The configuration menu, located on the right side of the screen, allows you to customize the following endpoint settings:

* **Region:** The geographical region where the model is hosted.
* **Model:** The specific AI model to be used for generating responses.
* **Temperature:** Controls the randomness of the generated response. Higher values increase the likelihood of more diverse and creative responses, while lower values result in more predictable and deterministic responses.
* **Max tokens:** The maximum number of tokens (words or characters) that the model will generate in its response.
* **Top p:** A parameter that controls the sampling of the next token. It represents the cumulative probability of the top tokens to be considered. A higher value means more tokens are considered, resulting in more diverse responses.
* **Top k:** Similar to Top p, but it directly limits the number of top tokens to be considered when generating the next token.
* **Stop:** A specific string that, when generated by the model, will cause the response to be truncated.
* **Presence penalty:** A penalty applied to tokens that have already been used in the response, discouraging the model from repeating them.
* **Frequency penalty:** A penalty applied to tokens based on their frequency of use in the response, discouraging the model from using the same tokens repeatedly.
* **View (UI or API):** The interface through which the model will be interacted with. UI refers to a user-friendly graphical interface, while API refers to a programmatic interface accessed via any combination of Python, Javascript, and cURL commands.
* **System prompt:** A predefined prompt or instruction that is passed to the model along with the user's input, influencing the tone and direction of the response.

### Using the Chat UI
In order to levrage CentML's Serverless endpoints via the Chat UI, follow the instructions below. 

1. Configure the endpoint settings to fit your testing needs using the options above.
2. Enter a prompt into the textbox at the bottom of the screen.
3. Select the arrow (pointing up) to send the prompt to the model.


<Frame>
    <img src="/images/chat.png" style={{ borderRadius: '0.5rem' }} />
</Frame>

### Accessing the Serverless Endpoint Programmatically 

In addition to the chat UI, you can access the serverless endpoint programmatically using Python, JavaScript, and cURL commands.

To get started, switch to the `API` under the `View` option on the right-hand side of the screen to find the corresponding code snippets.
<Frame> <img src="/images/api.png" style={{ borderRadius: '0.5rem' }} /> </Frame>


You can use the drop down menu to select the `Python`, `JavaScript`, or `Curl` options to view examples in those respective languages. 

Note that the editor is read-only. To execute the code snippets, you'll need to run the commands from a separate terminal on a local or remote machine.

We will provide examples of prompts in all supported languages in the following sections.

#### Using the Code Snippets (cURL)

To run the `curl` commands, copy the platform provided code into your terminal (with cURL installed) and execute it. Note that you must add your API token to the `curl` command

**Example `Curl` Command**
```
 curl -X POST "https://api.centml.com/openai/v1/chat/completions" \
  -H "Authorization: Bearer <your serverless token>" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
    "messages": [
      { "role": "system", "content": "You are a helpful assistant." },
      { "role": "user", "content": "What is CentMLs value prop?" }
    ],
    "max_tokens": 2000,
    "n": 1,
    "stream": false,
    "temperature": 0.7,
    "top_p": 1,
    "top_k": 40,
    "frequency_penalty": 0,
    "presence_penalty": 0.5,
    "stop": []
  }'
```
**Output**

<Frame>
    <img src="/images/curl-output.png" style={{ borderRadius: '0.5rem' }} />
</Frame>

#### Using the Code Snippets (Python)

In order to execute the Serverless UI provided Python script (should you opt into using Python), you must ensure you install the [OpenAPI Python library](https://github.com/openai/openai-python) by running `pip3 install openai` from their desired terminal or Python environment.
Once installed, save the Python script from the UI into a file (we named ours `centml-serverless.py`) and run it using `python3 <your file name>` (i.e. `python3 centml-serverless.py`). Note that you will have to ensure you've added your API token to the script.

**Example Python Script**

```
from openai import OpenAI

client = OpenAI(
    api_key="<Your API Key>",
    base_url="https://api.centml.com/openai/v1",
)

# Define your question
user_question = "How does CentML improve your AIOps?"

completion = client.chat.completions.create(
    model="meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": user_question}  # Add the user's question
    ],
    max_tokens=2000,
    temperature=0.7,
    top_p=1,
    n=1,
    stream=False,
    frequency_penalty=0,
    presence_penalty=0.5,
    stop=[]
)

print(completion.choices[0].message)

```
***Output***

<Frame> <img src="/images/python-output.png" style={{ borderRadius: '0.5rem' }} /> </Frame>

**NOTE:** The [CentML Python SDK and Client](https://github.com/CentML/centml-python-client/tree/main) do not interact with Serverless deployments. 
For a more robust deployment, we recommend using [LLM Serving](https://docs.centml.ai/apps/llm) and unlocking the full potential of the CentML platform.


#### Using the Code Snippets (JavaScript)

In order to leverage the platform provided JavaScript code snippets, you must install the `OpenAI` library. You can do so by running `npm install openai` or `yarn add openai` in your terminal.

Once the OpenAI library has been installed, save the platform provided code snippet in a file ending with `.js`. We named ours `index.js`.

Once you are ready to run the code, run the command `node <your file>.js` (i.e. `node index.js`). Note that you will have to ensure youâ€™ve added your API token to the script.

You should see an LLM response! 

**Example JavaScript Code**

```
import OpenAI from "openai";

const client = new OpenAI({
  apiKey: "<your API key",
  baseURL: "https://api.centml.com/openai/v1",
});

async function main() {
  const userPrompt = "How can CentML improve my time to value?"; // Change this to your desired command or prompt

  const completion = await client.chat.completions.create({
    model: "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
    messages: [
      { "role": "system", "content": "You are a helpful assistant." },
      { "role": "user", "content": userPrompt }, // Add the user's prompt or command here
    ],
    max_tokens: 2000,
    temperature: 0.7,
    top_p: 1,
    top_k: 40,
    n: 1,
    stream: false,
    frequency_penalty: 0,
    presence_penalty: 0.5,
    stop: []
  });

  console.log(completion.choices[0].message.content); // Log the response from the model
}

main();
```
**Output**
<Frame> <img src="/images/JavaScript-output.png" style={{ borderRadius: '0.5rem' }} /> </Frame>


#### `Invalid API Key`
Should your command return the `Invalid API key` error, ensure you are using your Serverless API key and not a different key from your vault. It may also make sense to ensure you have the proper syntax. Such as `BaseUrl` vs. `BaseURL`.

### Data Policies 

#### Prompt Training 
Currently CentML does not support prompt training via their Serverless endpoints. 

#### Prompt Logging
CentML does not log user prompts when they are submitted to their Serverless endpoints. Refreshing the browser window will reset the models memory. During a session, you can view submitted API calls via the `API` option on the right side of the Serverless UI.
This is the same location where example API call snippets are provided.

CentML does not currently provide a way for you to view or extract your chat histories. 

#### Moderation
CentML does not fine-tune or moderate models. However, CentML offers Llama Guard models to help you create a safer experience.

**Integrating Llama Guard with CentML**

You can integrate Llama Guard into your chat submission pipelines, utilizing CentML as your inferencing engine and AI infrastructure provider. For optimal performance and guaranteed SLAs, CentML recommends leveraging [LLM Serving](https://docs.centml.ai/apps/llm) for such integrations.


### Additional Considerations When Leveraging CentML's Serverless Endpoint Offering
#### Concurrency
When submitting requests to a CentML Serverless endpoint you may witness an `{"error":"Concurrent request limit reached, please try again later"}%` like message. When using the Chat UI or APIs directly, you are restricted to a limited number of requests based on demand. 
CentML Serverless endpoints are multi-user and not a dedicated deployment of a specific LLM nor a production chat application. The tool is designed to help you quickly test new models and collect some base performance metrics before moving to a dedicated endpoint such as [LLM Serving](https://docs.centml.ai/apps/llm). 
You may choose to use [OpenRouter](https://openrouter.ai/provider/centml) for a higher level of concurrency and guaranteed performance should you not want a dedicated endpoint.


#### Model Availability
CentML's Serverless offering only hosts a small subset of the available models out there on the market today. You can submit a request for new models (see below), but should you want to use a fine-tune or domain specific models, then Serverless endpoints are **not** the optimal solution. You might consider [LLM Serving](https://docs.centml.ai/apps/llm) as well as [General Inference](https://docs.centml.ai/apps/inference) for more robust use cases.

#### Token Limits 
When using the Serverless UI, you may notice a token limit option on the configuration menu on the right side of the screen, that is an artificial limit. Token limits are based on the upstream model limits and not limited when using the Serverless API directly via Python, JavaScript, or cURL commands.

### Requesting a New Model
As stated above, CentML does not host every possible (or even popular) model. The team's goal is to provide models for those looking to begin testing before moving to a dedicated endpoint. 
Should you want a specific model deployed to the CentML Serverless API/UI, you can request a new model by selecting `+ Request a model` from the configuration menu on the right side of the screen and filling out the form.

Once submitted, the CentML team will review the submitted request and respond in a timely manner.  

Please do not submit multiple tickets with the same model request. That will not expedite the process. Should you need escalation, feel free to reach out to the sales teams you've been engaged with or contact `sales@centml.ai`.

**Request a Model**
<Frame>
    <img src="/images/request-model-option.png" style={{ borderRadius: '0.5rem' }} />
</Frame>

**Submit a Ticket**
<Frame>
    <img src="/images/support-request.png" style={{ borderRadius: '0.5rem' }} />
</Frame>

#### Model Request Ticket Best Practices: What to Include
In the case of a feature request or a new model addition request, a good support ticket should include:
  * **Use case description:** A detailed enough explanation of the use case and how it aligns with the business goals.
  * **Requirements and constraints:** Any specific requirements or constraints related to the request, such as performance, scalability, or security considerations.
  * **Evaluation criteria:** Information on how the success of the request will be measured or evaluated.
  * **Current Limitations:** Details on how currently available models have failed to meet your requirements. 

Including all necessary elements in a support request enables the CentML team to prioritize and address issues in a timely manner.

#### Example New Model Support Request 

>Description:  
We are reaching out to request the addition of a new Large Language Model (LLM) to your serverless 
API. Our team is currently evaluating various LLMs for our natural language processing (NLP) use cases, and we believe that integrating this new model will enhance our capabilities.  
>
>Business Use Case:  
>
>Our primary use case is to leverage the new LLM for text classification, sentiment analysis, and language translation tasks. We anticipate that this model will provide more accurate results compared to our current models, enabling us to improve our customer experience and gain a competitive edge. Specifically, we plan to use the new model to:  
>
>* Analyze customer feedback and sentiment on our platform.  
>* Classify and route customer inquiries to the relevant support teams.  
>* Translate user-generated content to facilitate global communication.  
>
>Limitations:  
>
>While the current hosted LLMs show promise, our testing has revealed limitations that impact their suitability for our use cases. Specifically:  
>
>* Lack of domain-specific fine-tuning affects text classification accuracy.  
>* Sentiment analysis is biased towards certain emotional tones or language styles.  
>* Language translation struggles with domain-specific content, idioms, and colloquialisms.  
>
>We're concerned that these limitations may compromise the accuracy and reliability of our customer-facing applications. We're looking for alternative models or customizations that can address these issues.  
>
>Concurrency Considerations:  
>
>We understand that serverless APIs are designed to handle variable workloads, and we expect our usage to be moderate. Initially, we anticipate an average of 10 requests per minute, with occasional spikes to 50 requests per minute during peak hours. We believe that the serverless API can handle this concurrency level without significant performance degradation.  
>
>Future Plans:  
>
>After evaluating the performance of the new model, we anticipate that our usage will grow, and we may eventually require a dedicated endpoint to handle our workload. We are working towards assessing the model's performance and scalability, and we expect to migrate to a dedicated endpoint if our request volume exceeds 500 requests per minute. We would like to request guidance on the process for migrating to a dedicated endpoint, should it become necessary.

## Additional Support
If you need any additional support or have questions around CentML's Serverless endpoint offering, please do not hesitate to submit a support request by selecting `Support` located near the bottom of the sidebar menu.
There are several catagories to choose from, please select the category that is most applicable to your request. (see below)
<Frame>
    <img src="/images/general-request.png" style={{ borderRadius: '0.5rem' }} />
</Frame>

For best practices, follow similar rules, guidelines, and structure outlined within the `New Model Support Request` section above. This will ensure the CentML team understands the purpose and impacts of your request, and help the team triage in order to respond in a timely manner.

## Contact Sales 
For sales support, please feel free to follow the guidelines above, but use the `Sales Questions` category or reach out to `sales@centml.ai`

## Billing Questions
For billing support, please feel free to follow the guidelines above, but use the `Billing and Finance` category or reach out to `sales@centml.ai`

## Pricing
CentML Platform pricing can be found within the [Pricing section](https://docs.centml.ai/resources/pricing) of the documentation page.

### What next?

<CardGroup cols={2}>
  <Card
    title="Our Services"
    icon="box-open-full"
    href="/apps/llm"
  >
    Learn how to build your AI application using CentML Platform
  </Card>
  <Card
    title="Clients"
    icon="terminal"
    href="/clients/setup"
  >
    How to interact with Platform programmatically
  </Card>
  <Card
    title="Resouces and Pricing"
    icon="coins"
    href="/resources/pricing"
  >
    Learn more about CentML Platform
  </Card>
</CardGroup>
